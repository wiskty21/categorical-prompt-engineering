# -*- coding: utf-8 -*-
"""
æœ€é©åŒ–ã•ã‚ŒãŸåœè«–çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè£…
ãƒãƒƒãƒå‡¦ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’æ­è¼‰ã—ãŸå®Œå…¨ç‰ˆ

Features:
- ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹APIåŠ¹ç‡åŒ–
- ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå‡¦ç†
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- å‹•çš„ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
"""

import asyncio
import anthropic
import time
import hashlib
import pickle
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
from collections import OrderedDict, defaultdict
import json
import os
from dotenv import load_dotenv
import logging
from contextlib import asynccontextmanager
import threading
from concurrent.futures import ThreadPoolExecutor
import gc
import psutil
import weakref

# æœ€é©åŒ–ã•ã‚ŒãŸãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()
CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY")

if not CLAUDE_API_KEY:
    raise ValueError("CLAUDE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


@dataclass
class CacheConfig:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š"""
    max_size: int = 1000
    ttl_seconds: float = 3600.0  # 1æ™‚é–“
    enable_persistence: bool = False
    persistence_file: str = "cache.pkl"
    memory_limit_mb: int = 100


@dataclass
class BatchConfig:
    """ãƒãƒƒãƒå‡¦ç†è¨­å®š"""
    max_batch_size: int = 10
    batch_timeout: float = 2.0
    enable_adaptive_batching: bool = True
    min_batch_size: int = 2


@dataclass
class OptimizationConfig:
    """æœ€é©åŒ–è¨­å®š"""
    cache_config: CacheConfig = field(default_factory=CacheConfig)
    batch_config: BatchConfig = field(default_factory=BatchConfig)
    enable_memory_optimization: bool = True
    enable_performance_monitoring: bool = True
    max_concurrent_requests: int = 8
    adaptive_rate_control: bool = True


class LRUCache:
    """é«˜æ€§èƒ½LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…"""
    
    def __init__(self, max_size: int, ttl_seconds: float):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = OrderedDict()
        self.timestamps = {}
        self.lock = threading.RLock()
        self.hit_count = 0
        self.miss_count = 0
    
    def _hash_key(self, key: Any) -> str:
        """ã‚­ãƒ¼ã®ãƒãƒƒã‚·ãƒ¥åŒ–"""
        if isinstance(key, str):
            return hashlib.md5(key.encode()).hexdigest()
        else:
            return hashlib.md5(str(key).encode()).hexdigest()
    
    def _is_expired(self, key: str) -> bool:
        """æœŸé™åˆ‡ã‚Œãƒã‚§ãƒƒã‚¯"""
        if key not in self.timestamps:
            return True
        return time.time() - self.timestamps[key] > self.ttl_seconds
    
    def get(self, key: Any) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        with self.lock:
            hashed_key = self._hash_key(key)
            
            if hashed_key in self.cache and not self._is_expired(hashed_key):
                # LRUæ›´æ–°
                value = self.cache.pop(hashed_key)
                self.cache[hashed_key] = value
                self.hit_count += 1
                return value
            
            self.miss_count += 1
            return None
    
    def put(self, key: Any, value: Any) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        with self.lock:
            hashed_key = self._hash_key(key)
            
            # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã®æ›´æ–°
            if hashed_key in self.cache:
                self.cache.pop(hashed_key)
            
            # å®¹é‡ãƒã‚§ãƒƒã‚¯
            while len(self.cache) >= self.max_size:
                oldest_key, _ = self.cache.popitem(last=False)
                self.timestamps.pop(oldest_key, None)
            
            # æ–°ã‚¨ãƒ³ãƒˆãƒªè¿½åŠ 
            self.cache[hashed_key] = value
            self.timestamps[hashed_key] = time.time()
    
    def clear_expired(self) -> int:
        """æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤"""
        with self.lock:
            expired_keys = []
            for key in self.cache:
                if self._is_expired(key):
                    expired_keys.append(key)
            
            for key in expired_keys:
                self.cache.pop(key, None)
                self.timestamps.pop(key, None)
            
            return len(expired_keys)
    
    def stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ"""
        with self.lock:
            total_requests = self.hit_count + self.miss_count
            hit_rate = self.hit_count / total_requests if total_requests > 0 else 0
            
            return {
                "size": len(self.cache),
                "max_size": self.max_size,
                "hit_count": self.hit_count,
                "miss_count": self.miss_count,
                "hit_rate": hit_rate,
                "memory_usage_mb": self._estimate_memory_usage()
            }
    
    def _estimate_memory_usage(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®š"""
        try:
            return len(pickle.dumps(self.cache)) / (1024 * 1024)
        except:
            return 0.0


class BatchProcessor:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒãƒƒãƒå‡¦ç†"""
    
    def __init__(self, config: BatchConfig):
        self.config = config
        self.pending_requests = []
        self.batch_lock = asyncio.Lock()
        self.batch_event = asyncio.Event()
        self.processing = False
        self.metrics = {
            "total_batches": 0,
            "total_requests": 0,
            "average_batch_size": 0.0,
            "batch_efficiency": 0.0
        }
    
    async def add_request(self, request_data: Dict[str, Any]) -> Any:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒã«è¿½åŠ """
        future = asyncio.get_event_loop().create_future()
        
        async with self.batch_lock:
            self.pending_requests.append({
                "data": request_data,
                "future": future,
                "timestamp": time.time()
            })
            
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‹ã€ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–æ¡ä»¶ã‚’æº€ãŸã™å ´åˆ
            if (len(self.pending_requests) >= self.config.max_batch_size or
                (self.config.enable_adaptive_batching and 
                 self._should_process_batch())):
                
                if not self.processing:
                    asyncio.create_task(self._process_batch())
        
        return await future
    
    def _should_process_batch(self) -> bool:
        """ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã™ã¹ãã‹ã®åˆ¤å®š"""
        if len(self.pending_requests) < self.config.min_batch_size:
            return False
        
        # æœ€å¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãã†ãªå ´åˆ
        if self.pending_requests:
            oldest_time = self.pending_requests[0]["timestamp"]
            if time.time() - oldest_time >= self.config.batch_timeout:
                return True
        
        return False
    
    async def _process_batch(self):
        """ãƒãƒƒãƒã‚’å‡¦ç†"""
        async with self.batch_lock:
            if self.processing or not self.pending_requests:
                return
            
            self.processing = True
            batch = self.pending_requests.copy()
            self.pending_requests.clear()
        
        try:
            logger.info(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(batch)}ä»¶ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
            
            # å®Ÿéš›ã®ãƒãƒƒãƒå‡¦ç†ï¼ˆä¸¦è¡Œå®Ÿè¡Œï¼‰
            tasks = []
            for item in batch:
                task = self._process_single_request(item["data"])
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµæœã‚’å„Futureã«è¨­å®š
            for i, result in enumerate(results):
                future = batch[i]["future"]
                if isinstance(result, Exception):
                    future.set_exception(result)
                else:
                    future.set_result(result)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.metrics["total_batches"] += 1
            self.metrics["total_requests"] += len(batch)
            self.metrics["average_batch_size"] = (
                self.metrics["total_requests"] / self.metrics["total_batches"]
            )
            
            logger.info(f"ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(batch)}ä»¶")
            
        except Exception as e:
            logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…¨Futureã«ã‚¨ãƒ©ãƒ¼ã‚’è¨­å®š
            for item in batch:
                if not item["future"].done():
                    item["future"].set_exception(e)
        
        finally:
            self.processing = False
    
    async def _process_single_request(self, request_data: Dict[str, Any]) -> Any:
        """å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError()


class OptimizedClaudeClient:
    """æœ€é©åŒ–ã•ã‚ŒãŸClaude APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, api_key: str, config: OptimizationConfig = OptimizationConfig()):
        self.api_key = api_key
        self.config = config
        self.client = anthropic.AsyncAnthropic(api_key=api_key)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
        self.cache = LRUCache(
            config.cache_config.max_size,
            config.cache_config.ttl_seconds
        )
        
        # ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µ
        self.batch_processor = ClaudeBatchProcessor(
            config.batch_config,
            self.client
        )
        
        # ä¸¦è¡Œåˆ¶å¾¡
        self.semaphore = asyncio.Semaphore(config.max_concurrent_requests)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self.performance_monitor = PerformanceMonitor() if config.enable_performance_monitoring else None
        
        # ãƒ¡ãƒ¢ãƒªç®¡ç†
        self.memory_manager = MemoryManager() if config.enable_memory_optimization else None
        
        # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡
        self.rate_controller = AdaptiveRateController() if config.adaptive_rate_control else None
    
    async def generate_response(self, prompt: str, max_tokens: int = 1000, 
                              use_cache: bool = True, use_batch: bool = False,
                              operation_name: str = "generate_response") -> str:
        """æœ€é©åŒ–ã•ã‚ŒãŸå¿œç­”ç”Ÿæˆ"""
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if use_cache:
            cache_key = f"{prompt}:{max_tokens}"
            cached_result = self.cache.get(cache_key)
            if cached_result is not None:
                logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ [{operation_name}]")
                return cached_result
        
        # ãƒãƒƒãƒå‡¦ç†
        if use_batch:
            request_data = {
                "prompt": prompt,
                "max_tokens": max_tokens,
                "operation_name": operation_name
            }
            result = await self.batch_processor.add_request(request_data)
        else:
            # ç›´æ¥å‡¦ç†
            result = await self._direct_process(prompt, max_tokens, operation_name)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if use_cache and result:
            self.cache.put(cache_key, result)
        
        return result
    
    async def _direct_process(self, prompt: str, max_tokens: int, operation_name: str) -> str:
        """ç›´æ¥å‡¦ç†"""
        async with self.semaphore:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹
            if self.performance_monitor:
                self.performance_monitor.start_operation(operation_name)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡
            if self.rate_controller:
                await self.rate_controller.wait_if_needed()
            
            try:
                response = await self.client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=max_tokens,
                    messages=[{"role": "user", "content": prompt}]
                )
                
                result = response.content[0].text
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡æ›´æ–°
                if self.rate_controller:
                    self.rate_controller.record_success()
                
                return result
                
            except Exception as e:
                if self.rate_controller:
                    self.rate_controller.record_failure()
                raise
            
            finally:
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµ‚äº†
                if self.performance_monitor:
                    self.performance_monitor.end_operation(operation_name)
                
                # ãƒ¡ãƒ¢ãƒªç®¡ç†
                if self.memory_manager:
                    self.memory_manager.check_and_optimize()
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """æœ€é©åŒ–çµ±è¨ˆã‚’å–å¾—"""
        stats = {
            "cache_stats": self.cache.stats(),
            "batch_stats": self.batch_processor.metrics
        }
        
        if self.performance_monitor:
            stats["performance_stats"] = self.performance_monitor.get_stats()
        
        if self.rate_controller:
            stats["rate_control_stats"] = self.rate_controller.get_stats()
        
        if self.memory_manager:
            stats["memory_stats"] = self.memory_manager.get_stats()
        
        return stats
    
    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        # æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤
        expired_count = self.cache.clear_expired()
        logger.info(f"æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚’{expired_count}ä»¶å‰Šé™¤")
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        if self.memory_manager:
            self.memory_manager.force_cleanup()
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµ‚äº†
        await self.client.aclose()


class ClaudeBatchProcessor(BatchProcessor):
    """Claudeå°‚ç”¨ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µ"""
    
    def __init__(self, config: BatchConfig, client: anthropic.AsyncAnthropic):
        super().__init__(config)
        self.client = client
    
    async def _process_single_request(self, request_data: Dict[str, Any]) -> str:
        """Claude APIã®å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†"""
        try:
            response = await self.client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=request_data["max_tokens"],
                messages=[{"role": "user", "content": request_data["prompt"]}]
            )
            return response.content[0].text
        except Exception as e:
            logger.error(f"ãƒãƒƒãƒå†…ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            raise


class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–"""
    
    def __init__(self):
        self.operations = defaultdict(list)
        self.active_operations = {}
    
    def start_operation(self, operation_name: str) -> str:
        """æ“ä½œé–‹å§‹"""
        operation_id = f"{operation_name}_{time.time()}"
        self.active_operations[operation_id] = {
            "name": operation_name,
            "start_time": time.time(),
            "memory_before": self._get_memory_usage()
        }
        return operation_id
    
    def end_operation(self, operation_name: str) -> None:
        """æ“ä½œçµ‚äº†"""
        # æœ€æ–°ã®åŒåæ“ä½œã‚’çµ‚äº†
        for op_id, op_data in list(self.active_operations.items()):
            if op_data["name"] == operation_name:
                end_time = time.time()
                duration = end_time - op_data["start_time"]
                memory_after = self._get_memory_usage()
                
                self.operations[operation_name].append({
                    "duration": duration,
                    "memory_delta": memory_after - op_data["memory_before"],
                    "timestamp": end_time
                })
                
                del self.active_operations[op_id]
                break
    
    def _get_memory_usage(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # MB
        except:
            return 0.0
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆå–å¾—"""
        stats = {}
        
        for op_name, op_data in self.operations.items():
            if op_data:
                durations = [op["duration"] for op in op_data]
                memory_deltas = [op["memory_delta"] for op in op_data]
                
                stats[op_name] = {
                    "count": len(op_data),
                    "avg_duration": sum(durations) / len(durations),
                    "max_duration": max(durations),
                    "min_duration": min(durations),
                    "avg_memory_delta": sum(memory_deltas) / len(memory_deltas),
                    "total_duration": sum(durations)
                }
        
        return stats


class MemoryManager:
    """ãƒ¡ãƒ¢ãƒªç®¡ç†"""
    
    def __init__(self, threshold_mb: float = 500.0):
        self.threshold_mb = threshold_mb
        self.last_cleanup = time.time()
        self.cleanup_interval = 300  # 5åˆ†
    
    def check_and_optimize(self) -> None:
        """ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯ã¨æœ€é©åŒ–"""
        current_memory = self._get_memory_usage()
        
        if (current_memory > self.threshold_mb or 
            time.time() - self.last_cleanup > self.cleanup_interval):
            self.force_cleanup()
    
    def force_cleanup(self) -> None:
        """å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        before_memory = self._get_memory_usage()
        
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        collected = gc.collect()
        
        after_memory = self._get_memory_usage()
        freed_mb = before_memory - after_memory
        
        logger.info(f"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ: {collected}ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‰Šé™¤, "
                   f"{freed_mb:.1f}MBè§£æ”¾")
        
        self.last_cleanup = time.time()
    
    def _get_memory_usage(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # MB
        except:
            return 0.0
    
    def get_stats(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            
            return {
                "rss_mb": memory_info.rss / 1024 / 1024,
                "vms_mb": memory_info.vms / 1024 / 1024,
                "threshold_mb": self.threshold_mb,
                "last_cleanup": self.last_cleanup
            }
        except:
            return {}


class AdaptiveRateController:
    """ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡"""
    
    def __init__(self):
        self.success_count = 0
        self.failure_count = 0
        self.current_delay = 0.0
        self.last_request_time = 0.0
        self.target_success_rate = 0.95
        self.adjustment_factor = 1.2
        self.min_delay = 0.0
        self.max_delay = 5.0
    
    async def wait_if_needed(self) -> None:
        """å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
        if self.current_delay > 0:
            await asyncio.sleep(self.current_delay)
        
        self.last_request_time = time.time()
    
    def record_success(self) -> None:
        """æˆåŠŸè¨˜éŒ²"""
        self.success_count += 1
        self._adjust_rate()
    
    def record_failure(self) -> None:
        """å¤±æ•—è¨˜éŒ²"""
        self.failure_count += 1
        self._adjust_rate()
    
    def _adjust_rate(self) -> None:
        """ãƒ¬ãƒ¼ãƒˆèª¿æ•´"""
        total_requests = self.success_count + self.failure_count
        
        if total_requests > 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆ
            current_success_rate = self.success_count / total_requests
            
            if current_success_rate < self.target_success_rate:
                # æˆåŠŸç‡ãŒä½ã„å ´åˆã¯é…å»¶ã‚’å¢—åŠ 
                self.current_delay = min(
                    self.current_delay * self.adjustment_factor,
                    self.max_delay
                )
            else:
                # æˆåŠŸç‡ãŒé«˜ã„å ´åˆã¯é…å»¶ã‚’æ¸›å°‘
                self.current_delay = max(
                    self.current_delay / self.adjustment_factor,
                    self.min_delay
                )
    
    def get_stats(self) -> Dict[str, Any]:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡çµ±è¨ˆ"""
        total_requests = self.success_count + self.failure_count
        success_rate = self.success_count / total_requests if total_requests > 0 else 0
        
        return {
            "success_count": self.success_count,
            "failure_count": self.failure_count,
            "success_rate": success_rate,
            "current_delay": self.current_delay,
            "target_success_rate": self.target_success_rate
        }


# =============================================================================
# æœ€é©åŒ–ã•ã‚ŒãŸåœè«–çš„ã‚¯ãƒ©ã‚¹ç¾¤
# =============================================================================

class OptimizedTensorProduct:
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ç©"""
    
    def __init__(self, perspectives: List[str], integration_strategy: str = "synthesis",
                 client: OptimizedClaudeClient = None):
        self.perspectives = perspectives
        self.integration_strategy = integration_strategy
        self.client = client or OptimizedClaudeClient(CLAUDE_API_KEY)
    
    async def apply(self, input_text: str, use_cache: bool = True, 
                   use_batch: bool = True) -> Dict[str, Any]:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ç©å®Ÿè¡Œ"""
        logger.info(f"ğŸš€ æœ€é©åŒ–ãƒ†ãƒ³ã‚½ãƒ«ç©å®Ÿè¡Œ: {len(self.perspectives)}è¦³ç‚¹ "
                   f"(ã‚­ãƒ£ãƒƒã‚·ãƒ¥:{use_cache}, ãƒãƒƒãƒ:{use_batch})")
        
        start_time = time.time()
        
        # ä¸¦è¡Œåˆ†æ
        individual_results = await self._optimized_parallel_analysis(
            input_text, use_cache, use_batch
        )
        
        # æœ€é©åŒ–çµ±åˆ
        integrated_result = await self._optimized_integration(
            input_text, individual_results, use_cache
        )
        
        end_time = time.time()
        
        return {
            "input": input_text,
            "perspectives": self.perspectives,
            "individual_results": individual_results,
            "integrated_result": integrated_result,
            "processing_time": end_time - start_time,
            "optimization_stats": self.client.get_optimization_stats(),
            "optimized_processing": True
        }
    
    async def _optimized_parallel_analysis(self, input_text: str, 
                                         use_cache: bool, use_batch: bool) -> Dict[str, str]:
        """æœ€é©åŒ–ã•ã‚ŒãŸä¸¦è¡Œåˆ†æ"""
        tasks = []
        
        for perspective in self.perspectives:
            prompt = f"""
{perspective}ã®è¦³ç‚¹ã‹ã‚‰ä»¥ä¸‹ã‚’åˆ†æ:

å¯¾è±¡: {input_text}

{perspective}ã¨ã—ã¦é‡è¦ãªç‚¹:
1. æ ¸å¿ƒè¦ç´ 
2. ä¸»è¦èª²é¡Œ
3. å®Ÿè·µçš„ææ¡ˆ

åˆ†æçµæœ:
"""
            
            task = self.client.generate_response(
                prompt,
                max_tokens=800,
                use_cache=use_cache,
                use_batch=use_batch,
                operation_name=f"analyze_{perspective}"
            )
            tasks.append((perspective, task))
        
        # çµæœåé›†
        individual_results = {}
        for perspective, task in tasks:
            try:
                result = await task
                individual_results[perspective] = result
                logger.info(f"âœ… {perspective}åˆ†æå®Œäº†")
            except Exception as e:
                individual_results[perspective] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
                logger.warning(f"âš ï¸ {perspective}åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        return individual_results
    
    async def _optimized_integration(self, input_text: str, 
                                   individual_results: Dict[str, str],
                                   use_cache: bool) -> str:
        """æœ€é©åŒ–ã•ã‚ŒãŸçµ±åˆå‡¦ç†"""
        # æˆåŠŸã—ãŸçµæœã®ã¿çµ±åˆ
        valid_results = {k: v for k, v in individual_results.items() 
                        if not v.startswith("ã‚¨ãƒ©ãƒ¼")}
        
        integration_prompt = f"""
ã€Œ{input_text}ã€ã®å¤šè§’çš„åˆ†æã‚’çµ±åˆ:

"""
        
        for perspective, result in valid_results.items():
            integration_prompt += f"ã€{perspective}ã€‘{result[:300]}...\n\n"
        
        integration_prompt += "çµ±åˆè¦‹è§£:"
        
        return await self.client.generate_response(
            integration_prompt,
            max_tokens=1200,
            use_cache=use_cache,
            use_batch=False,  # çµ±åˆã¯å˜ç‹¬å‡¦ç†
            operation_name="integration"
        )


# =============================================================================
# å®Ÿæ¼”é–¢æ•°
# =============================================================================

async def demonstrate_optimization():
    """æœ€é©åŒ–æ©Ÿèƒ½ã®å®Ÿæ¼”"""
    print("ğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸåœè«–çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿæ¼”")
    print("=" * 80)
    
    # è¨­å®š
    config = OptimizationConfig(
        cache_config=CacheConfig(max_size=100, ttl_seconds=1800),
        batch_config=BatchConfig(max_batch_size=5, batch_timeout=1.0),
        enable_memory_optimization=True,
        enable_performance_monitoring=True,
        adaptive_rate_control=True
    )
    
    client = OptimizedClaudeClient(CLAUDE_API_KEY, config)
    
    input_topic = "æŒç¶šå¯èƒ½ãªéƒ½å¸‚é–‹ç™º"
    perspectives = ["ç’°å¢ƒ", "çµŒæ¸ˆ", "ç¤¾ä¼š", "æŠ€è¡“", "æ”¿ç­–"]
    
    tensor = OptimizedTensorProduct(perspectives, client=client)
    
    try:
        # åˆå›å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
        print("\nğŸ”„ åˆå›å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰")
        result1 = await tensor.apply(input_topic, use_cache=False, use_batch=True)
        
        print(f"å‡¦ç†æ™‚é–“: {result1['processing_time']:.2f}ç§’")
        
        # 2å›ç›®å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
        print("\nğŸ”„ 2å›ç›®å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰")
        result2 = await tensor.apply(input_topic, use_cache=True, use_batch=True)
        
        print(f"å‡¦ç†æ™‚é–“: {result2['processing_time']:.2f}ç§’")
        print(f"é«˜é€ŸåŒ–: {result1['processing_time']/result2['processing_time']:.1f}å€")
        
        # æœ€é©åŒ–çµ±è¨ˆ
        stats = client.get_optimization_stats()
        print("\nğŸ“Š æœ€é©åŒ–çµ±è¨ˆ:")
        
        if 'cache_stats' in stats:
            cache_stats = stats['cache_stats']
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_stats['hit_rate']:.1%}")
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {cache_stats['size']}/{cache_stats['max_size']}")
        
        if 'batch_stats' in stats:
            batch_stats = stats['batch_stats']
            print(f"ãƒãƒƒãƒå‡¦ç†æ•°: {batch_stats['total_batches']}")
            print(f"å¹³å‡ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_stats['average_batch_size']:.1f}")
        
        if 'performance_stats' in stats:
            perf_stats = stats['performance_stats']
            print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–: {len(perf_stats)}ç¨®é¡ã®æ“ä½œã‚’è¨˜éŒ²")
        
        # çµ±åˆçµæœã®è¡¨ç¤º
        print(f"\nğŸ¯ æœ€é©åŒ–ã•ã‚ŒãŸçµ±åˆçµæœ:")
        integrated = result2['integrated_result']
        print(integrated[:400] + "..." if len(integrated) > 400 else integrated)
        
        return result2
        
    finally:
        await client.cleanup()


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒŸ æœ€é©åŒ–ã•ã‚ŒãŸåœè«–çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹")
    
    try:
        result = await demonstrate_optimization()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ å®Ÿæ¼”å®Œäº†!")
        print("=" * 80)
        print("âœ… ãƒãƒƒãƒå‡¦ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’å®Ÿç¾!")
        print("ğŸš€ Phase 4: é«˜åº¦åŒ–ãƒ»æœ€é©åŒ– å®Œäº†!")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("å®Ÿæ¼”ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")


if __name__ == "__main__":
    asyncio.run(main())